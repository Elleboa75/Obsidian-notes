22092709:06
Status:  [[LanguageAndSpeech]]
Tags: 

---
# Speech perception
[[Speech perception]]
How are acoustic and visual speech signals mapped onto **language forms**

- Perception of pitch is relative for the whole sentence / context. 
- It is difficult to find reliable constant relations between *phonemes* and the *produced acoustic sounds*.
	- The variation is caused by **phonemic environment**.
	- Variation due to differing **speech conditions** (speech tempo, carefulness, etc)
	- Variation cause by **speaker variation** (physical properties, dialects and foreign accents)
- **Variability** carries important information, such as *indexical information*:
	- Talker identity
	- Physical characteristics
	- emotional state
	- socio-economical status
- People distinguish discrete sounds within some variation range. 
- **Phonemic constancy** â€“ sounds are distinctive despite lack of invariance

### Talker normalisation theories
- Assume auditory, perceptual or cognitive transformations of speech signals to map them onto speaker-independent categories
- **Stimulus intrinsic information**
	- speech samples contains sufficient information for accurate classification 
- **Stimulus extrinsic information**
	- Listeners build up talker specific mapping over multiple speech samples 
	- *Contextual tuning*: retuning phonetic mappings when talker changes are detected (or expected)

Challenges for speech perception:
- *Lack of invariance* problem
- *Perceptual constancy* and normalisation
- *Segmentation problem* 
	- difficult to identify and segment individual speech sounds.
	- Speech sounds are influenced by preceding and following sounds:
		- Influence across syllable and word boundaries
		- Speech sounds overlap
		- Not linear: speech sounds are influenced by later vowel or consonants

# Speech synthesis
	
### First generation speech synthesis
- Stevens & House (1955): Vocal tract as an acoustic tube

- **Articulatory synthesis**: Machine to mimic human speech system
- **Source-filter models**
	- Combining sound source with linear acoustic filter
- **Formant synthesizers**

```ad-todo

Listen to podcast on vocoder

```


### Vocoding
	Technique for coding speech more efficiently for long distance phone calls.
We measure a maximal signal at a given time to create *an envelope* of the wave. Then, we divide the signal into bands representing a given frequency. The more bands, the higher is quality. 
	With 5 bands, the words are (barely) understandable. ![[Pasted image 20221028200937.png|600]]
**Vocoder**:
 - **encoder**: coding the speech
	- spectral characteristics change slowly over time
	- Speech signal was sliced into 20ms intervals for which a frequency was analysed. Then each slice is divided into bands of different amplitudes. 
- **decoder**
	- Re-synthesizing speech
	- Initially only amplitude information was retained

- **A3 Scrambling** was a method of encoding radio-telephone calls in which frequency bands were rearranged and inverted. It could be decoded in real time by the enemy.
- **Sigsaly**
	- Developed by Homer
	- Also called *Project X* or *Green Hornet*
	- Encryption method for voice calls based on vocoding.
	- Noise was added to the recording. To encode the message the same exact noise had to be used.

- Source waveform is generated by **explicit model**

### Second generation speech synthesis
- parallel with more general sound synthesis.
	- *Sound synthesis*: generating sound from scratch using electronic hardware or software
- Source waveform is generated by **data model** or is **sample based**
- Limited flexibility
- Dependent on coverage of the data base

#### Concatenative Synthesis
Selecting and concatenating speech units from **speech data base**
- Based on natural speech samples
- Aims at eliminating problems caused by assuming over simplistic speech production model
- It is a memory demanding method.

*Unit selection*:
- There is a trade-off between *flexibility* and *natural speech*
Larger units | Phones, syllables and **diphones**
-- | --
+More natural speech | Not all phonemes exists in a language
-less generalisable | 
-more recordings necessary | 
```ad-todo

read about data gathering and synthesizing

```

### Third generation speech synthesis
	Source waveform is learned form the data
- Machine learning approaches 

- **Mel Frequency Cepstral Coefficients (MFCC)**
	1. Divide signal in frames of 20-40 ms
		- Identify frequencies present in the timebin
	2. Mel filterbank
		- Determine filterbank energies
	3. Log transform
	4. Compute discrete cosine transform (DCT)

Advantages:
- Automatically train the specifications-to-parameter

**Text to speech conversion**:
		- text
		- linguistic
		- generation
- Identify tokens in the text
	- Units for which there is a method for finding their pronunciation
- Subtasks:
	- **Tokenizing**: split the text into smaller chunks
		- Sentences or utterances
		- Prosodic boundaries and edge effects
	- **Normalisation**
		- How do we know whether 'read' is in past or present form
		- *Homographs*: same token may have alternate pronunciation in different contexts 
		- *Prosodic features* are hard to predict and replicate

**Linguistic analysis**:
- Phonemes: basic units of sound
- **Prosodic information**

### Cochlear implants
- typically have 16-22 channels 
- Use vocoding 
- Therefore a lot of intonation such as intonation, emotion, gender is missing from the heard sound. 
- The individual envelopes are represented as short pulses. 

**PSOLA**:
algorithm which modifies the length and pitch of a signal


---
# References