22092709:06
Status:  #LanguageAndSpeech
Tags: 

# speech synthesis

## Speech perception
[[Speech perception]]
How are acoustic and visual speech signals mapped onto **language forms**

- Perception of pitch is relative for the whole sentence / context. 
- It is difficult to find reliable constant relations between *phonemes* and the *produced acoustic sounds*.
	- The variation is caused by **phonemic environment**.
	- Variation due to differing **speech conditions** (speech tempo, carefulness, etc)
	- Variation cause by **speaker variation** (physical properties, dialects and foreign accents)
- Variability carries important information, such as *indexical information*:
	- Talker identity
	- Physical characteristics
	- emotional state
	- socio-economical status
- People distinguish discrete sounds within some variation range. 

***Talker normalisation theories***:
- Assume auditory, perceptual or cognitive transformations of speech signals to map them onto speaker-independent categories
- **Stimulus intrinsic infomation**
	- sth
- **Stimulus extrinsic information**
	- sth 

- Challenges for speech perception:
	- Speech constancy
	- *sth there*
	- Segmentation problem 
		- difficult to identify and segment individual speech sounds.
		- Speech sounds are influenced by preceding and following sounds:
			- Influence across syllable and word boundaries
			- Speech sounds overlap
			- Not linear: speech sounds are influenced by later vowel or consonants

## Speech synthesis
	
### First generation speech synthesis

- **Articulatory synthesis**: Machine to mimic human speech system
- **Source-filter models**: combining 
- **Formant synthesizers**

```ad-todo

Listen to podcast on vocoder

```


**Vocoding**: Technique for coding speech more efficiently for long distance phone calls.
We measure a maximal signal at a given time to create *an envelope* of the wave. Then, we divide the signal into bands representing a given frequency. The more bands, the higher is quality. 
	With 5 bands, the words are (barely) understandable. 
- Vocoder:
	- encoder: coding the speech
		- spectral characteristics change over time
		- Speech signal was sliced into 20ms intervals for which a frequency was analysed. Then each slice is divided into bands of different amplitudes. 

- *Sigsaly*
	- Encryption method for voice calls based on vocoding.

- Source waveform is generated by **explicit model**

### Second generation speech synthesis
- Source waveform is generated by **data model**
- Limited flexibility
- Dependent on coverage of the data base

==Unit selection:==
- There is a trade-off between *flexibility* and *natural speech*
Larger units | Phones, syllables and **diphones**
-- | --
+More natural speech | Not all phonemes exists in a language
-less generalizable | 
more recordings necessary | 
```ad-todo

read about data gathering and synthesizing

```

### Third generation speech synthesis
	Source waveform is learned form the data
- Machine learning approaches 

- **Mel filterbank**
	- Amplitudes are calculated separately for overlapping  frequencies.

Advantages:
- Automatically train the specifications-to-parameter

Text to speech conversion:
- Identify tokens in the text
	- Units for which...

**Normalisation**
- How do we know whether 'read' is in past or present form

**Linguistic analysis**:
- Phonemes: basic units of sound
- **Prosodic information**

### Cochlear implants
- typically have 16-22 channels 
- Use vocoding 
- Therefore a lot of intonation such as intonation, emotion, gender is missing from the heard sound. 
- The individual envelopes are represented as short pulses. 



---
# References